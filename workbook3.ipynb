{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA934 Numerical Methods - Workbook 3\n",
    "\n",
    "If you haven't already done so, install the DualNumbers Julia package. It is a good idea to update all your packages first. The commands are\n",
    "\n",
    ">Pkg.update()\n",
    "\n",
    ">Pkg.add(\"DualNumbers\")\n",
    "\n",
    "but you only need to run them once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pkg.update()\n",
    "#Pkg.add(\"DualNumbers\")\n",
    "using Plots\n",
    "using DualNumbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Numerical differentiation\n",
    "\n",
    "**1))** Derive a finite difference formula for the derivative of a function, $f$ at a point $x$ using the 3-point stencil $(x, x+h, x+2h)$ and state the order of the approximation error in terms of $h$.\n",
    "\n",
    "**2)** Write a formula for the derivative, $f^\\prime(x)$, of the function\n",
    "\n",
    "$$f(x) = \\sin(\\exp(x)) $$\n",
    "\n",
    "and evaluate it at $x=1$.\n",
    "\n",
    "**3)** Use your finite difference formula to approximate the value of $f^\\prime(1)$ for values of $h$ decreasing from $2^{-1}$ to $2^{-30}$ in powers of $2$. Plot the error as a function of $h$ and verify the theoretically predicted scaling of the error with $h$. What is the best relative error you can achieve?\n",
    "\n",
    "**4)** Read the examples at https://github.com/JuliaDiff/DualNumbers.jl. Define a dual number $x = 1+\\epsilon$ and use it to evaluate $f^\\prime(1)$. Verify that the answer is accurate to within machine precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1) \n",
    "\n",
    "We evaluate the function f(x) at the points of the stencil and taylor expand the function around these points:\n",
    "\n",
    "\\begin{align}\n",
    "f(x)&=f(x)\\\\\n",
    "f(x+h)&=f(x)+f'(x)h+f''(x)\\frac{h^2}{2}+\\mathcal{O}(h^3)\\\\\n",
    "f(x+2h)&=f(x)+2f'(x)h+2f''(x){h^2}+\\mathcal{O}(h^3)\n",
    "\\end{align}\n",
    "\n",
    "We define F as a linear combination of the stencil: \n",
    "\n",
    " $$F= A f(x) + B f(x+h) + C f(x+2h)$$\n",
    "\n",
    "\n",
    "We want to use $F$ to approximate $f'(x)$, so we have to solve the following system of linear equations: \n",
    "\n",
    "\\begin{align}\n",
    "A+B+C&=0\\\\\n",
    "B+2C&=1\\\\\n",
    "\\frac{B}{2}+2C&=0\n",
    "\\end{align}\n",
    "\n",
    "Solving this gives $ A=-3/2 , B=2 , C=-1/2$\n",
    "\n",
    "Finally, $F=f'(x)h+\\mathcal{O}(h^3)$. Rearranging this suggests that \n",
    "\n",
    "$$ f'(x) = \\frac{-3/2f(x)+2f(x+h)-1/2f(x+2h)}{h}+\\mathcal{O}(h^2)$$\n",
    "\n",
    "Therefore the order of the approximation error is $h^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Finding roots\n",
    "\n",
    "**1)** Referring to the function, $f(x)$, defined above, find the roots of the equation\n",
    "\n",
    "$$ f(x) = 0$$\n",
    "\n",
    "in the interval $0<x<2$.\n",
    "\n",
    "**2)** Implement the bracketing and bisection method to find one of the roots numerically. Measure the error at each iteration of the algorithm and demonstrate that the error decreases exponentially as a function of the number of iterations. To how many digits of precision can you approximate the root?\n",
    "\n",
    "**3)** Perform the same measurements for the Newton Raphson method and show that the error decreases faster than exponentially as a function of the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Finding minima\n",
    "\n",
    "**1)** The function $f(x)$ above has a single minimum in the interval $0<x<2$. Find its location analytically.\n",
    "\n",
    "**2)** Implement the Golden section search to find the location of this minimum numerically. Plot the error as a function of the number of iterations. To how many digits of precision can you approximate the location of the minimum?\n",
    "\n",
    "**3)** To understand your empirical findings, use Taylor's Theorem to show that near a minimum, $x_*$, of f(x),\n",
    "\n",
    "$$f(x) \\approx f(x_*)\\left( 1+ \\frac{f^{\\prime\\prime}(x_*)}{2\\,f(x_*)}\\,(x-x_*)^2\\right). $$\n",
    "Show that in order for a computer to distinguish between $f(x)$ and $f(x_*)$ we must have\n",
    "\n",
    "$$ \\left| x-x_*\\right| > \\sqrt{\\epsilon_m}\\,\\sqrt{\\left|\\frac{2\\,f(x_*)}{f^{\\prime\\prime}(x_*)}\\right|}$$\n",
    "\n",
    "thus limiting the precision with which the location of a minimum can be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
